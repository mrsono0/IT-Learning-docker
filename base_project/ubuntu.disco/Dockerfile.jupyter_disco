FROM mrsono0/base_project:base_dev_disco

########################################################################
# Set up OS
########################################################################
USER root
EXPOSE 8888 443
WORKDIR /root

ENV CPPFLAGS=-s \
    SHELL=/bin/bash

CMD /usr/local/bin/start_jupyter.sh --notebook-dir=/workspace --port=8888 --no-browser --ip=0.0.0.0 --NotebookApp.token=

COPY ubuntu/start_jupyter.sh /usr/local/bin/
RUN mkdir -p /workspace

ENV DEBIAN_FRONTEND noninteractive
RUN sed -i "s/archive.ubuntu.com/mirror.kakao.com/g" /etc/apt/sources.list
# RUN sed -i "s/archive.canonical.com/mirror.kakao.com/g" /etc/apt/sources.list
RUN sed -i "s/security.ubuntu.com/mirror.kakao.com/g" /etc/apt/sources.list
RUN sed -i "s/# deb-src/deb-src/g" /etc/apt/sources.list
RUN apt-get -y update --fix-missing
RUN apt-get -yy upgrade
RUN \
    apt-get -y install binutils && \
    apt-get -y install \
    file \
    make \
    cmake \
    cython \
    libunwind-dev \
    # openjdk-12-jdk \
    # openjdk-12-jre-headless \
    # ca-certificates-java \
    fonts-liberation && \
    rm -rf /var/lib/apt/lists/*

# ENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64
# ENV PATH $JAVA_HOME/bin:$PATH
# RUN echo "export JAVA_HOME=${JAVA_HOME}" >> /etc/profile && \
#     echo "export PATH=${JAVA_HOME}/bin:$PATH" >> /etc/profile && \
#     ln -sfT "$JAVA_HOME" /usr/java/default && \
#     ln -sfT "$JAVA_HOME" /usr/java/latest

# HADOOP
ENV HADOOP_VERSION 3.2.0
ENV HADOOP_HOME /usr/hadoop-$HADOOP_VERSION
ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
ENV PATH $PATH:$HADOOP_HOME/bin
RUN ln -fs $HADOOP_HOME /usr/hadoop
RUN echo "export HADOOP_HOME=${HADOOP_HOME}" >> /etc/profile
RUN echo "export HADOOP_CONF_DIR=${HADOOP_CONF_DIR}" >> /etc/profile
RUN echo "export PATH=${HADOOP_HOME}/bin:$PATH" >> /etc/profile
RUN curl -sL --retry 3 \
    "http://mirror.apache-kr.org/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz" \
    | gunzip \
    | tar -x -C /usr/
    # rm -rf $HADOOP_HOME/share/doc

# SPARK spark-2.4.0-bin-without-hadoop
ENV SPARK_VERSION 2.4.0
ENV SPARK_PACKAGE spark-$SPARK_VERSION-bin-without-hadoop
ENV SPARK_HOME /usr/spark-$SPARK_VERSION
ENV PYSPARK_PYTHON python3:/usr/spark/python/lib/py4j-0.10.7-src.zip
ENV PYSPARK_DRIVER_PYTHON jupyter 
ENV PYSPARK_DRIVER_PYTHON_OPTS "lab --notebook-dir=/workspace --port=8888 --no-browser --allow-root --ip=0.0.0.0 --NotebookApp.token="
ENV PYTHONPATH $SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip
ENV SPARK_DIST_CLASSPATH="$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*"
ENV PATH $PATH:$SPARK_HOME/bin
RUN ln -fs $SPARK_HOME /usr/spark
RUN echo "export SPARK_HOME=${SPARK_HOME}" >> /etc/profile
RUN echo "export PYSPARK_PYTHON=${PYSPARK_PYTHON}" >> /etc/profile
RUN echo "export PYSPARK_DRIVER_PYTHON=${PYSPARK_DRIVER_PYTHON}" >> /etc/profile
RUN echo 'export PYSPARK_DRIVER_PYTHON_OPTS="lab --notebook-dir=/workspace --port=8888 --no-browser --allow-root --ip=0.0.0.0 --NotebookApp.token="'  >> /etc/profile
RUN echo "export PYTHONPATH=${PYTHONPATH}" >> /etc/profile
RUN echo "export SPARK_DIST_CLASSPATH=${SPARK_DIST_CLASSPATH}" >> /etc/profile
RUN echo "export PATH=${SPARK_HOME}/bin:$PATH" >> /etc/profile
RUN curl -sL --retry 3 \
    "http://mirror.apache-kr.org/spark/spark-$SPARK_VERSION/$SPARK_PACKAGE.tgz" \
    | gunzip \
    | tar -x -C /usr/ && \
    mv /usr/$SPARK_PACKAGE $SPARK_HOME
    # rm -rf $SPARK_HOME/examples $SPARK_HOME/ec2

# Spark and Mesos config
# ENV SPARK_HOME /usr/local/spark
# ENV PYSPARK_PYTHON python3:/usr/spark/python/lib/py4j-0.10.7-src.zip
# ENV PYSPARK_DRIVER_PYTHON ipython 
# ENV SPARK_DIST_CLASSPATH="$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*"
# ENV PATH $PATH:$SPARK_HOME/bin
# ENV PYTHONPATH $SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip
# ENV MESOS_NATIVE_LIBRARY /usr/local/lib/libmesos.so
# ENV SPARK_OPTS --driver-java-options=-Xms1024M --driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=info
# ENV APACHE_SPARK_VERSION 2.4.0
# ENV HADOOP_VERSION 2.7
# RUN cd /tmp && \
#         wget -q http://mirrors.ukfast.co.uk/sites/ftp.apache.org/spark/spark-${APACHE_SPARK_VERSION}/spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
#         echo "5F4184E0FE7E5C8AE67F5E6BC5DEEE881051CC712E9FF8AEDDF3529724C00E402C94BB75561DD9517A372F06C1FCB78DC7AE65DCBD4C156B3BA4D8E267EC2936 *spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" | sha512sum -c - && \
#         tar xzf spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -C /usr/local --owner root --group root --no-same-owner && \
#         rm spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz
# RUN cd /usr/local && ln -s spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} spark
COPY ubuntu/mesos.key /tmp/
RUN apt-get -y update && \
    apt-get install --no-install-recommends -y gnupg libunwind-dev && \
    apt-get install libcurl4 libcurl4-openssl-dev -y && \
    apt-key add /tmp/mesos.key && \
    # echo "deb http://repos.mesosphere.io/ubuntu xenial main" > /etc/apt/sources.list.d/mesosphere.list && \
    apt-get -y update && \
    apt-get --no-install-recommends -y install mesos\* && \
    # apt-get purge --auto-remove -y gnupg && \
    rm -rf /var/lib/apt/lists/*
ENV MESOS_NATIVE_LIBRARY /usr/local/lib/libmesos.so
ENV SPARK_OPTS "--driver-java-options=-Xms1024M --driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=info"
RUN echo "export MESOS_NATIVE_LIBRARY=${MESOS_NATIVE_LIBRARY}" >> /etc/profile
RUN echo "export SPARK_OPTS='${SPARK_OPTS}'" >> /etc/profile

RUN conda update conda -y
RUN conda update -n base -c defaults conda -y
RUN pip3 install --no-cache-dir --upgrade setuptools pip

RUN conda config --system --prepend channels conda-forge && \
    conda config --system --set auto_update_conda false && \
    conda config --system --set show_channel_urls true && \
    # conda install --quiet --yes conda="${MINICONDA_VERSION%.*}.*" && \
    conda update --all --quiet --yes && \
    # conda build purge-all && \
    conda clean -tipsy
    # rm -rf /home/$NB_USER/.cache/yarn
RUN conda install --quiet -y -c conda-forge \
    pyarrow && \
    # tensorflow \
    # keras  && \
    conda clean -tipsy 
RUN conda install --quiet --yes \
    'notebook=5.7.4' \
    'jupyterhub=0.9.4' \
    'jupyterlab=0.35.4' && \
    conda clean -tipsy && \
    jupyter labextension install @jupyterlab/hub-extension@^0.12.0 && \
    npm cache clean --force && \
    jupyter notebook --generate-config && \
    rm -rf $CONDA_DIR/share/jupyter/lab/staging
    # rm -rf /home/$NB_USER/.cache/yarn
RUN pip3 install --no-cache-dir --upgrade \
    numpy \
    pandas \
    pandas-datareader \
    pandas-td \
    ipywidgets \
    jupyter_dashboards \
    pypki2 \
    ipydeps \
    ordo \
    beakerx \
    bash_kernel \
    sparkmagic
RUN conda install -c conda-forge \
    cling \
    xeus-cling
RUN beakerx install
RUN python3 -m bash_kernel.install
RUN curl -fsSLO "https://github.com/SpencerPark/IJava/releases/download/v1.2.0/ijava-1.2.0.zip" && \
    unzip ijava-1.2.0.zip && \
    python3 install.py --sys-prefix && \
    rm -rf java ijava-1.2.0.zip install.py
RUN npm config set user 0 && \
    npm config set unsafe-perm true && \
    npm install -g ijavascript && \
    ijsinstall
RUN npm install -g itypescript && \
    its --ts-install=local
RUN pip3 install http://github.com/nbgallery/nbgallery-extensions/tarball/master#egg=jupyter_nbgallery && \
    echo "### Install jupyter extensions" && \
    jupyter serverextension enable --py jupyterlab && \
    jupyter nbextension enable --py --sys-prefix widgetsnbextension && \
    jupyter serverextension enable --py jupyter_nbgallery && \
    jupyter nbextension install --py jupyter_nbgallery && \
    jupyter nbextension enable jupyter_nbgallery --py && \
    jupyter dashboards quick-setup --sys-prefix && \
    jupyter nbextension install --py ordo && \
    jupyter nbextension enable ordo --py

# RUN jupyter-kernelspec install sparkmagic/kernels/sparkkernel && \
#     jupyter-kernelspec install sparkmagic/kernels/pysparkkernel && \
#     jupyter-kernelspec install sparkmagic/kernels/pyspark3kernel && \
#     jupyter-kernelspec install sparkmagic/kernels/sparkrkernel

RUN rm -rf /opt/conda/share/jupyter/kernels/clojure && \
    rm -rf /opt/conda/share/jupyter/kernels/groovy
    # rm -rf /opt/conda/share/jupyter/kernels/kotlin
    # rm -rf /opt/conda/share/jupyter/kernels/scala

RUN conda install -c conda-forge jupyter_contrib_nbextensions

RUN apt-get update && apt-get install -yq --no-install-recommends \
    emacs \
    inkscape \
    jed \
    libsm6 \
    libxext-dev \
    libxrender1 \
    lmodern \
    netcat \
    pandoc \
    python3-dev \
    texlive-fonts-extra \
    texlive-fonts-recommended \
    texlive-generic-recommended \
    texlive-latex-base \
    texlive-latex-extra \
    texlive-xetex \
    unzip \
    nano \
    && rm -rf /var/lib/apt/lists/*

ENV GOPATH=/go
ENV PATH=$PATH:$JAVA_HOME/bin:$SPARK_HOME/bin:$GOPATH/bin:/opt/conda/share/jupyter/kernels/installers \
    LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$JAVA_HOME/jre/lib/amd64/server

RUN echo "export PATH=$PATH" >> /etc/profile
